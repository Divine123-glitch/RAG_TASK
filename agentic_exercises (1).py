{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Patterns: Exercise Solutions\n",
    "\n",
    "This notebook contains solutions to the three practice exercises from Notebook 04.\n",
    "\n",
    "**Exercises:**\n",
    "1. Enhanced ReAct with Reasoning Traces\n",
    "2. Plan-Execute with Replanning\n",
    "3. Adaptive Reflection with Quality Metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal, TypedDict, Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=openai_api_key)\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools used across exercises\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculate mathematical expressions.\n",
    "    \n",
    "    Args:\n",
    "        expression: Math expression like \"2 + 2\" or \"15 * 37\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information (simulated).\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "    \"\"\"\n",
    "    knowledge = {\n",
    "        \"python\": \"Python is a high-level programming language created in 1991.\",\n",
    "        \"langgraph\": \"LangGraph is a framework for building stateful multi-actor applications.\",\n",
    "        \"react\": \"ReAct is an agent pattern that combines reasoning and acting.\",\n",
    "        \"ai\": \"Artificial Intelligence is the simulation of human intelligence by machines.\"\n",
    "    }\n",
    "    \n",
    "    for key, value in knowledge.items():\n",
    "        if key in query.lower():\n",
    "            return value\n",
    "    \n",
    "    return \"No information found.\"\n",
    "\n",
    "print(\"‚úÖ Tools defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Enhanced ReAct with Reasoning Traces\n",
    "\n",
    "**Goal:** Create a ReAct agent that explicitly shows its reasoning before each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom state for ReAct with reasoning\n",
    "class ReActWithReasoningState(TypedDict):\n",
    "    \"\"\"State that tracks messages and reasoning traces.\"\"\"\n",
    "    messages: Annotated[list, operator.add]\n",
    "    reasoning_traces: Annotated[list[str], operator.add]\n",
    "\n",
    "print(\"‚úÖ State defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the think tool\n",
    "@tool\n",
    "def think(reasoning: str) -> str:\n",
    "    \"\"\"\n",
    "    Record your reasoning before taking action.\n",
    "    Call this BEFORE using any other tool.\n",
    "    \n",
    "    Args:\n",
    "        reasoning: Your thought process and plan\n",
    "    \"\"\"\n",
    "    return f\"Recorded reasoning: {reasoning}\"\n",
    "\n",
    "# All tools including think\n",
    "reasoning_tools = [think, calculator, search]\n",
    "\n",
    "print(\"‚úÖ Think tool added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Enhanced ReAct Agent\n",
    "llm_with_reasoning_tools = llm.bind_tools(reasoning_tools)\n",
    "\n",
    "system_prompt = SystemMessage(content=\"\"\"You are a helpful assistant with tools.\n",
    "\n",
    "IMPORTANT: Before using calculator or search, you MUST call the 'think' tool to explain your reasoning.\n",
    "\n",
    "Example:\n",
    "1. Call think(\"I need to calculate 2+2, so I'll use the calculator\")\n",
    "2. Call calculator(\"2+2\")\n",
    "3. Call think(\"I got the result, now I can answer\")\n",
    "\n",
    "Always show your thought process!\"\"\")\n",
    "\n",
    "def reasoning_assistant(state: ReActWithReasoningState) -> dict:\n",
    "    \"\"\"Agent node with reasoning.\"\"\"\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm_with_reasoning_tools.invoke(messages)\n",
    "    \n",
    "    # Extract reasoning if think tool was called\n",
    "    reasoning = []\n",
    "    if response.tool_calls:\n",
    "        for tc in response.tool_calls:\n",
    "            if tc['name'] == 'think':\n",
    "                reasoning.append(tc['args']['reasoning'])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"reasoning_traces\": reasoning\n",
    "    }\n",
    "\n",
    "def should_continue_reasoning(state: ReActWithReasoningState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Route to tools or end.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# Build graph\n",
    "reasoning_builder = StateGraph(ReActWithReasoningState)\n",
    "reasoning_builder.add_node(\"assistant\", reasoning_assistant)\n",
    "reasoning_builder.add_node(\"tools\", ToolNode(reasoning_tools))\n",
    "\n",
    "reasoning_builder.add_edge(START, \"assistant\")\n",
    "reasoning_builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    should_continue_reasoning,\n",
    "    {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "reasoning_builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "enhanced_react_agent = reasoning_builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"‚úÖ Enhanced ReAct agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Enhanced ReAct\n",
    "def test_enhanced_react(query: str):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    result = enhanced_react_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"reasoning_traces\": []\n",
    "        },\n",
    "        config={\"configurable\": {\"thread_id\": \"test_reasoning\"}}\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüß† REASONING TRACES:\")\n",
    "    for i, trace in enumerate(result[\"reasoning_traces\"], 1):\n",
    "        print(f\"{i}. {trace}\")\n",
    "    \n",
    "    final_answer = result[\"messages\"][-1].content\n",
    "    print(f\"\\n‚úÖ FINAL ANSWER:\\n{final_answer}\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Test 1\n",
    "test_enhanced_react(\"What is 25 * 4?\")\n",
    "\n",
    "# Test 2\n",
    "test_enhanced_react(\"Search for Python and calculate 100 / 5\")\n",
    "\n",
    "# Test 3\n",
    "test_enhanced_react(\"Tell me about LangGraph, then calculate 7 ** 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Plan-Execute with Replanning\n",
    "\n",
    "**Goal:** Add failure handling and replanning capability to Plan-Execute pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom state for Plan-Execute with replanning\n",
    "class PlanExecuteReplanState(TypedDict):\n",
    "    \"\"\"State for plan-execute with replanning.\"\"\"\n",
    "    input: str\n",
    "    plan: list[str]\n",
    "    original_plan: list[str]  # Store original for comparison\n",
    "    current_step: int\n",
    "    results: Annotated[list[str], operator.add]\n",
    "    replanned: bool  # Track if we've replanned\n",
    "    final_output: str\n",
    "\n",
    "print(\"‚úÖ Replanning state defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes for Plan-Execute with Replanning\n",
    "def planner_with_replan(state: PlanExecuteReplanState) -> dict:\n",
    "    \"\"\"Create initial plan.\"\"\"\n",
    "    prompt = f\"\"\"Create a step-by-step plan for this task:\n",
    "\n",
    "Task: {state['input']}\n",
    "\n",
    "Return a numbered list of concrete steps. Keep it simple (3-5 steps).\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    lines = response.content.split('\\n')\n",
    "    steps = [line.strip() for line in lines if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "    \n",
    "    print(f\"\\nüìã ORIGINAL PLAN:\")\n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"plan\": steps,\n",
    "        \"original_plan\": steps,\n",
    "        \"current_step\": 0,\n",
    "        \"results\": [],\n",
    "        \"replanned\": False\n",
    "    }\n",
    "\n",
    "def executor_with_failure(state: PlanExecuteReplanState) -> dict:\n",
    "    \"\"\"Execute current step, can fail.\"\"\"\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return {}\n",
    "    \n",
    "    current_step = state[\"plan\"][state[\"current_step\"]]\n",
    "    print(f\"‚öôÔ∏è Executing: {current_step}\")\n",
    "    \n",
    "    # Execute step\n",
    "    prompt = f\"\"\"Previous results: {state.get('results', [])}\\n\\nExecute this step: {current_step}\n",
    "    \n",
    "If this step would fail (e.g., division by zero), respond with 'FAILED: reason'.\n",
    "Otherwise, complete the step normally.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    result_content = response.content\n",
    "    \n",
    "    # Check for failure\n",
    "    if \"FAILED:\" in result_content.upper():\n",
    "        print(f\"‚ùå Step failed: {result_content}\\n\")\n",
    "        result = f\"Step {state['current_step'] + 1} FAILED: {result_content}\"\n",
    "    else:\n",
    "        print(f\"‚úì Done\\n\")\n",
    "        result = f\"Step {state['current_step'] + 1} result: {result_content}\"\n",
    "    \n",
    "    return {\n",
    "        \"results\": [result],\n",
    "        \"current_step\": state[\"current_step\"] + 1\n",
    "    }\n",
    "\n",
    "def replanner(state: PlanExecuteReplanState) -> dict:\n",
    "    \"\"\"Create new plan based on failure.\"\"\"\n",
    "    print(\"\\nüîÑ REPLANNING due to failure...\\n\")\n",
    "    \n",
    "    prompt = f\"\"\"The original plan failed. Create a new plan.\n",
    "\n",
    "Original task: {state['input']}\n",
    "Original plan: {state['original_plan']}\n",
    "Results so far: {state['results']}\n",
    "\n",
    "Create a NEW plan that addresses the failure. Return numbered steps.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    lines = response.content.split('\\n')\n",
    "    new_steps = [line.strip() for line in lines if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "    \n",
    "    print(f\"üìã REVISED PLAN:\")\n",
    "    for step in new_steps:\n",
    "        print(f\"  {step}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"plan\": new_steps,\n",
    "        \"current_step\": 0,\n",
    "        \"replanned\": True,\n",
    "        \"results\": []  # Reset results for new plan\n",
    "    }\n",
    "\n",
    "def finalizer_replan(state: PlanExecuteReplanState) -> dict:\n",
    "    \"\"\"Create final output.\"\"\"\n",
    "    prompt = f\"\"\"Combine these results into a final answer:\n",
    "\n",
    "Original task: {state['input']}\n",
    "Replanned: {state['replanned']}\n",
    "Results: {state['results']}\n",
    "\n",
    "Provide a clear final answer.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"final_output\": response.content}\n",
    "\n",
    "print(\"‚úÖ Replanning nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing functions\n",
    "def check_for_failure(state: PlanExecuteReplanState) -> Literal[\"replanner\", \"executor\", \"finalizer\"]:\n",
    "    \"\"\"Check if we need to replan due to failure.\"\"\"\n",
    "    # Check if last result was a failure\n",
    "    if state[\"results\"] and \"FAILED\" in state[\"results\"][-1]:\n",
    "        # Only replan once\n",
    "        if not state[\"replanned\"]:\n",
    "            return \"replanner\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Already replanned once, proceeding to finalizer\\n\")\n",
    "            return \"finalizer\"\n",
    "    \n",
    "    # Continue execution if more steps\n",
    "    if state[\"current_step\"] < len(state[\"plan\"]):\n",
    "        return \"executor\"\n",
    "    \n",
    "    return \"finalizer\"\n",
    "\n",
    "# Build graph\n",
    "replan_builder = StateGraph(PlanExecuteReplanState)\n",
    "\n",
    "replan_builder.add_node(\"planner\", planner_with_replan)\n",
    "replan_builder.add_node(\"executor\", executor_with_failure)\n",
    "replan_builder.add_node(\"replanner\", replanner)\n",
    "replan_builder.add_node(\"finalizer\", finalizer_replan)\n",
    "\n",
    "replan_builder.add_edge(START, \"planner\")\n",
    "replan_builder.add_edge(\"planner\", \"executor\")\n",
    "replan_builder.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    check_for_failure,\n",
    "    {\n",
    "        \"executor\": \"executor\",\n",
    "        \"replanner\": \"replanner\",\n",
    "        \"finalizer\": \"finalizer\"\n",
    "    }\n",
    ")\n",
    "replan_builder.add_edge(\"replanner\", \"executor\")\n",
    "replan_builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "replan_agent = replan_builder.compile()\n",
    "\n",
    "print(\"‚úÖ Plan-Execute with Replanning agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Replanning\n",
    "result = replan_agent.invoke({\n",
    "    \"input\": \"Calculate 10/0, then explain the result\"\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìä FINAL OUTPUT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result[\"final_output\"])\n",
    "print(f\"\\nReplanned: {result['replanned']}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Adaptive Reflection with Quality Metrics\n",
    "\n",
    "**Goal:** Improve Reflection pattern with structured quality scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for quality scores\n",
    "class QualityScores(BaseModel):\n",
    "    \"\"\"Structured quality evaluation.\"\"\"\n",
    "    clarity: int = Field(description=\"How clear is the response? (1-5)\", ge=1, le=5)\n",
    "    completeness: int = Field(description=\"How complete is the response? (1-5)\", ge=1, le=5)\n",
    "    accuracy: int = Field(description=\"How accurate is the response? (1-5)\", ge=1, le=5)\n",
    "    explanation: str = Field(description=\"Brief explanation of scores\")\n",
    "    \n",
    "    def is_approved(self) -> bool:\n",
    "        \"\"\"Check if all scores meet threshold.\"\"\"\n",
    "        return self.clarity >= 4 and self.completeness >= 4 and self.accuracy >= 4\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Clarity={self.clarity}, Completeness={self.completeness}, Accuracy={self.accuracy}\"\n",
    "\n",
    "print(\"‚úÖ Quality scores model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State for adaptive reflection\n",
    "class AdaptiveReflectionState(TypedDict):\n",
    "    \"\"\"State for adaptive reflection.\"\"\"\n",
    "    task: str\n",
    "    draft: str\n",
    "    scores_history: Annotated[list[QualityScores], operator.add]\n",
    "    iterations: int\n",
    "    final_output: str\n",
    "\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "print(\"‚úÖ Adaptive reflection state defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes for adaptive reflection\n",
    "llm_with_structured = llm.bind_tools([QualityScores])\n",
    "\n",
    "def adaptive_generator(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Generate or refine based on scores.\"\"\"\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"\"\"Create a response for this task:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Provide a clear, complete, and accurate answer.\"\"\"\n",
    "        print(\"\\n‚úèÔ∏è Generating initial draft...\")\n",
    "    else:\n",
    "        last_scores = state[\"scores_history\"][-1]\n",
    "        prompt = f\"\"\"Improve this draft based on the quality scores:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current draft: {state['draft']}\n",
    "\n",
    "Scores: {last_scores}\n",
    "Explanation: {last_scores.explanation}\n",
    "\n",
    "Focus on improving the lowest scores. Create an improved version.\"\"\"\n",
    "        print(f\"\\n‚úèÔ∏è Refining (iteration {state['iterations']})...\")\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"‚úì Draft created\\n\")\n",
    "    \n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "def adaptive_critic(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Evaluate with structured scores.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate this response with specific scores:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Response: {state['draft']}\n",
    "\n",
    "Rate on:\n",
    "- Clarity (1-5): How easy to understand?\n",
    "- Completeness (1-5): Does it fully answer the task?\n",
    "- Accuracy (1-5): Is the information correct?\n",
    "\n",
    "Use the QualityScores tool to provide structured feedback.\"\"\"\n",
    "    \n",
    "    print(\"üîç Evaluating draft...\")\n",
    "    response = llm_with_structured.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Extract scores from tool call\n",
    "    if response.tool_calls:\n",
    "        scores_data = response.tool_calls[0]['args']\n",
    "        scores = QualityScores(**scores_data)\n",
    "    else:\n",
    "        # Fallback if no tool call\n",
    "        scores = QualityScores(\n",
    "            clarity=3,\n",
    "            completeness=3,\n",
    "            accuracy=3,\n",
    "            explanation=\"Unable to extract structured scores\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Approved: {scores.is_approved()}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"scores_history\": [scores],\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "def adaptive_finalizer(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Finalize output.\"\"\"\n",
    "    print(\"\\n‚úÖ Reflection complete!\\n\")\n",
    "    return {\"final_output\": state[\"draft\"]}\n",
    "\n",
    "print(\"‚úÖ Adaptive reflection nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing function\n",
    "def should_refine(state: AdaptiveReflectionState) -> Literal[\"generator\", \"finalizer\"]:\n",
    "    \"\"\"Decide if refinement needed.\"\"\"\n",
    "    if not state[\"scores_history\"]:\n",
    "        return \"generator\"\n",
    "    \n",
    "    last_scores = state[\"scores_history\"][-1]\n",
    "    \n",
    "    # Stop if approved\n",
    "    if last_scores.is_approved():\n",
    "        print(\"‚úÖ All scores ‚â• 4, approved!\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    # Stop if max iterations\n",
    "    if state[\"iterations\"] >= MAX_ITERATIONS:\n",
    "        print(f\"‚ö†Ô∏è Max iterations ({MAX_ITERATIONS}) reached\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    return \"generator\"\n",
    "\n",
    "# Build graph\n",
    "adaptive_builder = StateGraph(AdaptiveReflectionState)\n",
    "\n",
    "adaptive_builder.add_node(\"generator\", adaptive_generator)\n",
    "adaptive_builder.add_node(\"critic\", adaptive_critic)\n",
    "adaptive
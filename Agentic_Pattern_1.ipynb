{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Patterns: Exercise Solutions\n",
    "\n",
    "This notebook contains solutions to the three practice exercises:\n",
    "1. Enhanced ReAct with Reasoning Traces\n",
    "2. Plan-Execute with Replanning\n",
    "3. Adaptive Reflection with Quality Metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal, TypedDict, Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "import os\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n",
    "\n",
    "print(\"âœ… API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"âœ… LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Enhanced ReAct with Reasoning Traces\n",
    "\n",
    "**Goal:** Create a ReAct agent that explicitly shows its reasoning process.\n",
    "\n",
    "**Key Features:**\n",
    "- Add a `think` tool for logging reasoning\n",
    "- Agent must think before taking actions\n",
    "- Store and display full reasoning chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom state to track reasoning\n",
    "class ReasoningState(TypedDict):\n",
    "    \"\"\"State that includes reasoning traces.\"\"\"\n",
    "    messages: Annotated[list, operator.add]\n",
    "    reasoning_trace: Annotated[list[str], operator.add]  # Track thoughts\n",
    "\n",
    "print(\"âœ… ReasoningState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools including think tool\n",
    "@tool\n",
    "def think(reasoning: str) -> str:\n",
    "    \"\"\"\n",
    "    Record your reasoning before taking action.\n",
    "    \n",
    "    Args:\n",
    "        reasoning: Your thought process and reasoning\n",
    "    \"\"\"\n",
    "    return f\"ðŸ’­ Thought recorded: {reasoning}\"\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculate mathematical expressions.\n",
    "    \n",
    "    Args:\n",
    "        expression: Math expression like \"2 + 2\" or \"15 * 37\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information (simulated).\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "    \"\"\"\n",
    "    knowledge = {\n",
    "        \"python\": \"Python is a high-level programming language created in 1991.\",\n",
    "        \"langgraph\": \"LangGraph is a framework for building stateful multi-actor applications.\",\n",
    "        \"react\": \"ReAct is an agent pattern that combines reasoning and acting.\"\n",
    "    }\n",
    "    \n",
    "    for key, value in knowledge.items():\n",
    "        if key in query.lower():\n",
    "            return value\n",
    "    \n",
    "    return \"No information found.\"\n",
    "\n",
    "reasoning_tools = [think, calculator, search]\n",
    "print(\"âœ… Tools with 'think' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Enhanced ReAct Agent\n",
    "llm_with_tools = llm.bind_tools(reasoning_tools)\n",
    "\n",
    "system_prompt = SystemMessage(content=\"\"\"You are a helpful assistant with tools.\n",
    "\n",
    "IMPORTANT: Before using calculator or search, you MUST first use the 'think' tool \n",
    "to explain your reasoning. Follow this pattern:\n",
    "\n",
    "1. Use 'think' to explain what you plan to do and why\n",
    "2. Use the appropriate action tool (calculator/search)\n",
    "3. Use 'think' again to explain the result\n",
    "4. Provide final answer\n",
    "\n",
    "Always show your reasoning process clearly.\"\"\")\n",
    "\n",
    "def reasoning_assistant(state: ReasoningState) -> dict:\n",
    "    \"\"\"Agent that reasons before acting.\"\"\"\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Extract reasoning from think tool calls\n",
    "    new_reasoning = []\n",
    "    if response.tool_calls:\n",
    "        for tc in response.tool_calls:\n",
    "            if tc[\"name\"] == \"think\":\n",
    "                new_reasoning.append(tc[\"args\"][\"reasoning\"])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"reasoning_trace\": new_reasoning\n",
    "    }\n",
    "\n",
    "def should_continue_reasoning(state: ReasoningState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Route to tools or end.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# Build graph\n",
    "reasoning_builder = StateGraph(ReasoningState)\n",
    "reasoning_builder.add_node(\"assistant\", reasoning_assistant)\n",
    "reasoning_builder.add_node(\"tools\", ToolNode(reasoning_tools))\n",
    "\n",
    "reasoning_builder.add_edge(START, \"assistant\")\n",
    "reasoning_builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    should_continue_reasoning,\n",
    "    {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "reasoning_builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "reasoning_agent = reasoning_builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… Enhanced ReAct agent with reasoning traces created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Enhanced ReAct Agent\n",
    "def test_reasoning_agent(query: str, test_num: int):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ§ª TEST {test_num}: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    result = reasoning_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"reasoning_trace\": []\n",
    "        },\n",
    "        config={\"configurable\": {\"thread_id\": f\"test_{test_num}\"}}\n",
    "    )\n",
    "    \n",
    "    # Display reasoning trace\n",
    "    if result[\"reasoning_trace\"]:\n",
    "        print(\"\\nðŸ’­ REASONING TRACE:\")\n",
    "        for i, thought in enumerate(result[\"reasoning_trace\"], 1):\n",
    "            print(f\"\\n  Thought {i}: {thought}\")\n",
    "    \n",
    "    # Display final answer\n",
    "    final_answer = result[\"messages\"][-1].content\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… FINAL ANSWER:\")\n",
    "    print(final_answer)\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Run 3 multi-step tests\n",
    "test_reasoning_agent(\n",
    "    \"Calculate 15 * 25, then search for information about Python\",\n",
    "    1\n",
    ")\n",
    "\n",
    "test_reasoning_agent(\n",
    "    \"What is LangGraph? Then calculate 100 / 4\",\n",
    "    2\n",
    ")\n",
    "\n",
    "test_reasoning_agent(\n",
    "    \"Calculate 2 ** 10, explain what ReAct is, then calculate 50 + 50\",\n",
    "    3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Analysis\n",
    "\n",
    "**Key Improvements:**\n",
    "- âœ… Explicit reasoning before each action\n",
    "- âœ… Reasoning traces stored and displayed\n",
    "- âœ… More transparent decision-making\n",
    "\n",
    "**Verbosity Comparison:**\n",
    "- Standard ReAct: Actions + Results\n",
    "- Enhanced ReAct: Thoughts + Actions + Results + Thoughts\n",
    "- ~2x more verbose but much clearer reasoning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Plan-Execute with Replanning\n",
    "\n",
    "**Goal:** Add failure handling and replanning capabilities.\n",
    "\n",
    "**Key Features:**\n",
    "- Executor can fail with reason\n",
    "- Replanner creates new plan on failure\n",
    "- Max 1 replan attempt\n",
    "- Show original and revised plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State for Plan-Execute with Replanning\n",
    "class ReplanState(TypedDict):\n",
    "    \"\"\"State for plan-execute with replanning.\"\"\"\n",
    "    input: str\n",
    "    plan: list[str]\n",
    "    original_plan: list[str]  # Track original for comparison\n",
    "    current_step: int\n",
    "    results: Annotated[list[str], operator.add]\n",
    "    failed_step: str  # Track which step failed\n",
    "    failure_reason: str\n",
    "    replan_count: int  # Limit replanning\n",
    "    final_output: str\n",
    "\n",
    "MAX_REPLANS = 1\n",
    "\n",
    "print(\"âœ… ReplanState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes for Plan-Execute with Replanning\n",
    "def planner_node(state: ReplanState) -> dict:\n",
    "    \"\"\"Create initial plan.\"\"\"\n",
    "    prompt = f\"\"\"Create a step-by-step plan for this task:\n",
    "\n",
    "Task: {state['input']}\n",
    "\n",
    "Return a numbered list of concrete steps. Keep it simple (3-5 steps).\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    lines = response.content.split('\\n')\n",
    "    steps = [line.strip() for line in lines if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ ORIGINAL PLAN CREATED:\")\n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"plan\": steps,\n",
    "        \"original_plan\": steps,\n",
    "        \"current_step\": 0,\n",
    "        \"results\": [],\n",
    "        \"replan_count\": 0\n",
    "    }\n",
    "\n",
    "def executor_node(state: ReplanState) -> dict:\n",
    "    \"\"\"Execute current step, can fail.\"\"\"\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return {}\n",
    "    \n",
    "    current_step = state[\"plan\"][state[\"current_step\"]]\n",
    "    \n",
    "    print(f\"âš™ï¸ Executing: {current_step}\")\n",
    "    \n",
    "    # Check for division by zero in the step\n",
    "    if \"10/0\" in current_step or \"divide by zero\" in current_step.lower():\n",
    "        print(f\"âŒ Step FAILED: Division by zero error\\n\")\n",
    "        return {\n",
    "            \"failed_step\": current_step,\n",
    "            \"failure_reason\": \"Cannot divide by zero - mathematical error\",\n",
    "            \"current_step\": state[\"current_step\"] + 1\n",
    "        }\n",
    "    \n",
    "    # Execute step normally\n",
    "    prompt = f\"\"\"Previous results: {state.get('results', [])}\\n\\nExecute this step: {current_step}\"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    result = f\"Step {state['current_step'] + 1} result: {response.content}\"\n",
    "    print(f\"âœ” Done\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"results\": [result],\n",
    "        \"current_step\": state[\"current_step\"] + 1\n",
    "    }\n",
    "\n",
    "def replanner_node(state: ReplanState) -> dict:\n",
    "    \"\"\"Create new plan based on failure.\"\"\"\n",
    "    prompt = f\"\"\"The original plan failed. Create a revised plan.\n",
    "\n",
    "Original task: {state['input']}\n",
    "Original plan: {state['original_plan']}\n",
    "Failed step: {state['failed_step']}\n",
    "Failure reason: {state['failure_reason']}\n",
    "Results so far: {state['results']}\n",
    "\n",
    "Create a NEW plan that avoids this failure. Return numbered steps.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    lines = response.content.split('\\n')\n",
    "    new_steps = [line.strip() for line in lines if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "    \n",
    "    print(f\"\\nðŸ”„ REVISED PLAN CREATED:\")\n",
    "    for step in new_steps:\n",
    "        print(f\"  {step}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"plan\": new_steps,\n",
    "        \"current_step\": 0,\n",
    "        \"failed_step\": \"\",\n",
    "        \"failure_reason\": \"\",\n",
    "        \"replan_count\": state[\"replan_count\"] + 1\n",
    "    }\n",
    "\n",
    "def finalizer_node(state: ReplanState) -> dict:\n",
    "    \"\"\"Create final output.\"\"\"\n",
    "    prompt = f\"\"\"Combine these results into a final answer:\n",
    "\n",
    "Original task: {state['input']}\n",
    "Results: {state['results']}\n",
    "\n",
    "Provide a clear, concise final answer.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"final_output\": response.content}\n",
    "\n",
    "print(\"âœ… Plan-Execute with replanning nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing functions\n",
    "def check_for_failure(state: ReplanState) -> Literal[\"replanner\", \"executor\", \"finalizer\"]:\n",
    "    \"\"\"Check if step failed and decide next action.\"\"\"\n",
    "    # If there's a failure and we haven't replanned yet\n",
    "    if state.get(\"failure_reason\") and state[\"replan_count\"] < MAX_REPLANS:\n",
    "        print(f\"\\nâš ï¸ Failure detected! Initiating replanning...\\n\")\n",
    "        return \"replanner\"\n",
    "    \n",
    "    # If all steps done\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    # Continue execution\n",
    "    return \"executor\"\n",
    "\n",
    "# Build graph\n",
    "replan_builder = StateGraph(ReplanState)\n",
    "\n",
    "replan_builder.add_node(\"planner\", planner_node)\n",
    "replan_builder.add_node(\"executor\", executor_node)\n",
    "replan_builder.add_node(\"replanner\", replanner_node)\n",
    "replan_builder.add_node(\"finalizer\", finalizer_node)\n",
    "\n",
    "replan_builder.add_edge(START, \"planner\")\n",
    "replan_builder.add_edge(\"planner\", \"executor\")\n",
    "replan_builder.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    check_for_failure,\n",
    "    {\n",
    "        \"replanner\": \"replanner\",\n",
    "        \"executor\": \"executor\",\n",
    "        \"finalizer\": \"finalizer\"\n",
    "    }\n",
    ")\n",
    "replan_builder.add_edge(\"replanner\", \"executor\")\n",
    "replan_builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "replan_agent = replan_builder.compile()\n",
    "\n",
    "print(\"âœ… Plan-Execute with replanning agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with failing scenario\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸ§ª TESTING PLAN-EXECUTE WITH REPLANNING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "result = replan_agent.invoke({\n",
    "    \"input\": \"Calculate 10/0, then explain the result\"\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸ“Š COMPARISON:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOriginal Plan: {result['original_plan']}\")\n",
    "print(f\"\\nRevised Plan: {result['plan']}\")\n",
    "print(f\"\\nReplan Count: {result['replan_count']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… FINAL OUTPUT:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(result[\"final_output\"])\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Analysis\n",
    "\n",
    "**Key Features Implemented:**\n",
    "- âœ… Failure detection in executor\n",
    "- âœ… Replanner creates adaptive plan\n",
    "- âœ… Max 1 replan prevents infinite loops\n",
    "- âœ… Original vs revised plan comparison\n",
    "\n",
    "**Replanning Decision:**\n",
    "When division by zero is detected:\n",
    "1. Executor returns failure reason\n",
    "2. Router directs to replanner (if replan_count < 1)\n",
    "3. Replanner creates new approach (explain error instead of calculate)\n",
    "4. Execution continues with revised plan\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Adaptive Reflection with Quality Metrics\n",
    "\n",
    "**Goal:** Improve reflection with structured quality scoring.\n",
    "\n",
    "**Key Features:**\n",
    "- Score drafts on multiple criteria (1-5)\n",
    "- Use Pydantic for structured output\n",
    "- Only refine if any score < 4\n",
    "- Track improvements across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for quality scoring\n",
    "class QualityScore(BaseModel):\n",
    "    \"\"\"Structured quality assessment.\"\"\"\n",
    "    clarity: int = Field(\n",
    "        description=\"How clear and understandable is the response? (1-5)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    completeness: int = Field(\n",
    "        description=\"Does it fully address the task? (1-5)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    accuracy: int = Field(\n",
    "        description=\"Is the information correct? (1-5)\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"Brief explanation of scores and suggestions for improvement\"\n",
    "    )\n",
    "    approved: bool = Field(\n",
    "        description=\"True if all scores >= 4, False otherwise\"\n",
    "    )\n",
    "\n",
    "# State for adaptive reflection\n",
    "class AdaptiveReflectionState(TypedDict):\n",
    "    \"\"\"State with quality metrics tracking.\"\"\"\n",
    "    task: str\n",
    "    draft: str\n",
    "    scores: List[QualityScore]  # Track score history\n",
    "    iterations: int\n",
    "    final_output: str\n",
    "\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "print(\"âœ… QualityScore model and AdaptiveReflectionState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM with structured output\n",
    "llm_with_scoring = llm.with_structured_output(QualityScore)\n",
    "\n",
    "print(\"âœ… LLM with structured output created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes for adaptive reflection\n",
    "def adaptive_generator(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Generate or refine based on scores.\"\"\"\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"\"\"Create a response for this task:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Provide a clear, complete, and accurate answer.\"\"\"\n",
    "        print(\"\\nâœï¸ Generating initial draft...\")\n",
    "    else:\n",
    "        last_score = state[\"scores\"][-1]\n",
    "        prompt = f\"\"\"Improve this draft based on the quality assessment:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current draft: {state['draft']}\n",
    "\n",
    "Quality Scores:\n",
    "- Clarity: {last_score.clarity}/5\n",
    "- Completeness: {last_score.completeness}/5\n",
    "- Accuracy: {last_score.accuracy}/5\n",
    "\n",
    "Feedback: {last_score.explanation}\n",
    "\n",
    "Create an improved version addressing the weaknesses.\"\"\"\n",
    "        print(f\"\\nâœï¸ Refining (iteration {state['iterations']})...\")\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"âœ” Draft created\\n\")\n",
    "    \n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "def adaptive_critic(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Score draft on multiple criteria.\"\"\"\n
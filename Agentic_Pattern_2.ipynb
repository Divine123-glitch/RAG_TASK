{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal, TypedDict, Annotated\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Adaptive Reflection with Quality Metrics\n",
    "\n",
    "Improve Reflection pattern with numerical quality scoring using Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for quality scoring\n",
    "class QualityScore(BaseModel):\n",
    "    \"\"\"Quality scores for draft evaluation.\"\"\"\n",
    "    clarity: int = Field(description=\"How clear and understandable is the response? (1-5)\", ge=1, le=5)\n",
    "    completeness: int = Field(description=\"How complete and thorough is the response? (1-5)\", ge=1, le=5)\n",
    "    accuracy: int = Field(description=\"How accurate and correct is the response? (1-5)\", ge=1, le=5)\n",
    "    feedback: str = Field(description=\"Specific feedback on what needs improvement\")\n",
    "    \n",
    "    def average_score(self) -> float:\n",
    "        return (self.clarity + self.completeness + self.accuracy) / 3\n",
    "    \n",
    "    def needs_refinement(self) -> bool:\n",
    "        \"\"\"Check if any score is below 4.\"\"\"\n",
    "        return min(self.clarity, self.completeness, self.accuracy) < 4\n",
    "\n",
    "print(\"✅ QualityScore model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State for adaptive reflection\n",
    "class AdaptiveReflectionState(TypedDict):\n",
    "    \"\"\"State for reflection with quality metrics.\"\"\"\n",
    "    task: str\n",
    "    draft: str\n",
    "    scores: Annotated[list[QualityScore], operator.add]\n",
    "    iterations: int\n",
    "    final_output: str\n",
    "\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "print(\"AdaptiveReflectionState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind structured output to LLM\n",
    "llm_with_scoring = llm.with_structured_output(QualityScore)\n",
    "\n",
    "# Node 1: Generator\n",
    "def adaptive_generator(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Generate or refine based on quality scores.\"\"\"\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"\"\"Create a response for this task:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Provide a clear, complete, and accurate answer.\"\"\"\n",
    "        print(\"\\nGenerating initial draft...\")\n",
    "    else:\n",
    "        last_score = state[\"scores\"][-1]\n",
    "        prompt = f\"\"\"Improve this draft based on the quality feedback:\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current draft: {state['draft']}\n",
    "\n",
    "Quality Scores:\n",
    "- Clarity: {last_score.clarity}/5\n",
    "- Completeness: {last_score.completeness}/5\n",
    "- Accuracy: {last_score.accuracy}/5\n",
    "\n",
    "Feedback: {last_score.feedback}\n",
    "\n",
    "Create an improved version that addresses the feedback and improves the scores.\"\"\"\n",
    "        print(f\"\\nRefining (iteration {state['iterations']})...\")\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"✓ Draft created\\n\")\n",
    "    \n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "# Node 2: Critic with scoring\n",
    "def adaptive_critic(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Evaluate draft and provide quality scores.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate this response using the following criteria (score 1-5 each):\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Response: {state['draft']}\n",
    "\n",
    "Criteria:\n",
    "1. Clarity: How clear and understandable is it?\n",
    "2. Completeness: How thorough is it?\n",
    "3. Accuracy: How correct and reliable is it?\n",
    "\n",
    "For each criterion, give a score from 1-5:\n",
    "- 5: Excellent\n",
    "- 4: Good\n",
    "- 3: Acceptable\n",
    "- 2: Needs improvement\n",
    "- 1: Poor\n",
    "\n",
    "Also provide specific feedback on what needs improvement.\"\"\"\n",
    "    \n",
    "    print(\"Evaluating draft...\")\n",
    "    score = llm_with_scoring.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    print(f\" Scores: Clarity={score.clarity}, Completeness={score.completeness}, Accuracy={score.accuracy}\")\n",
    "    print(f\" Feedback: {score.feedback[:100]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"scores\": [score],\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "# Node 3: Finalizer\n",
    "def adaptive_finalizer(state: AdaptiveReflectionState) -> dict:\n",
    "    \"\"\"Finalize output and show score progression.\"\"\"\n",
    "    print(\"\\n Reflection complete!\\n\")\n",
    "    print(\" Score Progression:\")\n",
    "    for i, score in enumerate(state[\"scores\"]):\n",
    "        print(f\"  Iteration {i+1}: Clarity={score.clarity}, Completeness={score.completeness}, Accuracy={score.accuracy} (Avg: {score.average_score():.2f})\")\n",
    "    print()\n",
    "    \n",
    "    return {\"final_output\": state[\"draft\"]}\n",
    "\n",
    "print(\" Adaptive reflection nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing based on quality scores\n",
    "def should_refine(state: AdaptiveReflectionState) -> Literal[\"generator\", \"finalizer\"]:\n",
    "    \"\"\"Decide if refinement needed based on scores.\"\"\"\n",
    "    if not state.get(\"scores\"):\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    last_score = state[\"scores\"][-1]\n",
    "    \n",
    "    # Stop if all scores >= 4\n",
    "    if not last_score.needs_refinement():\n",
    "        print(\" All quality scores >= 4. Approved!\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if state[\"iterations\"] >= MAX_ITERATIONS:\n",
    "        print(f\" Max iterations ({MAX_ITERATIONS}) reached\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    return \"generator\"\n",
    "\n",
    "# Build graph\n",
    "adaptive_builder = StateGraph(AdaptiveReflectionState)\n",
    "\n",
    "adaptive_builder.add_node(\"generator\", adaptive_generator)\n",
    "adaptive_builder.add_node(\"critic\", adaptive_critic)\n",
    "adaptive_builder.add_node(\"finalizer\", adaptive_finalizer)\n",
    "\n",
    "adaptive_builder.add_edge(START, \"generator\")\n",
    "adaptive_builder.add_edge(\"generator\", \"critic\")\n",
    "adaptive_builder.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_refine,\n",
    "    {\"generator\": \"generator\", \"finalizer\": \"finalizer\"}\n",
    ")\n",
    "adaptive_builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "adaptive_reflection_agent = adaptive_builder.compile()\n",
    "\n",
    "print(\" Adaptive reflection agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph\n",
    "try:\n",
    "    display(Image(adaptive_reflection_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Adaptive Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a task that requires refinement\n",
    "result = adaptive_reflection_agent.invoke({\n",
    "    \"task\": \"Explain quantum computing to a 10-year-old in 3-4 sentences\",\n",
    "    \"draft\": \"\",\n",
    "    \"scores\": [],\n",
    "    \"iterations\": 0\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\" FINAL OUTPUT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result[\"final_output\"])\n",
    "print(f\"\\nTotal iterations: {result['iterations']}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score improvements\n",
    "def visualize_scores(scores: list[QualityScore]):\n",
    "    \"\"\"Plot score improvements across iterations.\"\"\"\n",
    "    iterations = list(range(1, len(scores) + 1))\n",
    "    clarity = [s.clarity for s in scores]\n",
    "    completeness = [s.completeness for s in scores]\n",
    "    accuracy = [s.accuracy for s in scores]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, clarity, marker='o', label='Clarity', linewidth=2)\n",
    "    plt.plot(iterations, completeness, marker='s', label='Completeness', linewidth=2)\n",
    "    plt.plot(iterations, accuracy, marker='^', label='Accuracy', linewidth=2)\n",
    "    plt.axhline(y=4, color='green', linestyle='--', alpha=0.5, label='Target (4.0)')\n",
    "    \n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title('Quality Score Progression', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 5.5)\n",
    "    plt.xticks(iterations)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scores(result[\"scores\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Exercise 2: Plan-Execute + Reflection Hybrid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid state combining both patterns\n",
    "class HybridState(TypedDict):\n",
    "    \"\"\"Combined Plan-Execute and Reflection state.\"\"\"\n",
    "    # Plan-Execute components\n",
    "    input: str\n",
    "    plan: list[str]\n",
    "    current_step: int\n",
    "    results: Annotated[list[str], operator.add]\n",
    "    \n",
    "    # Reflection components\n",
    "    draft: str\n",
    "    critique: str\n",
    "    reflection_iterations: int\n",
    "    \n",
    "    # Final output\n",
    "    final_output: str\n",
    "\n",
    "MAX_REFLECTIONS = 2\n",
    "\n",
    "print(\" HybridState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Planner\n",
    "def hybrid_planner(state: HybridState) -> dict:\n",
    "    \"\"\"Create step-by-step plan.\"\"\"\n",
    "    prompt = f\"\"\"Create a step-by-step plan for this task:\n",
    "\n",
    "Task: {state['input']}\n",
    "\n",
    "Return a numbered list of 3-5 concrete steps.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse steps\n",
    "    lines = response.content.split('\\n')\n",
    "    steps = [line.strip() for line in lines if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" PLAN CREATED:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\"plan\": steps, \"current_step\": 0, \"results\": []}\n",
    "\n",
    "# Node 2: Executor\n",
    "def hybrid_executor(state: HybridState) -> dict:\n",
    "    \"\"\"Execute current step.\"\"\"\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return {}\n",
    "    \n",
    "    current_step = state[\"plan\"][state[\"current_step\"]]\n",
    "    \n",
    "    print(f\"\\n EXECUTING STEP {state['current_step'] + 1}:\")\n",
    "    print(f\"   {current_step}\")\n",
    "    \n",
    "    # Execute step\n",
    "    prompt = f\"\"\"Execute this step:\n",
    "\n",
    "Overall task: {state['input']}\n",
    "\n",
    "Previous results: {state.get('results', [])}\n",
    "\n",
    "Current step: {current_step}\n",
    "\n",
    "Provide a detailed result for this step.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    result = f\"Step {state['current_step'] + 1}: {response.content}\"\n",
    "    \n",
    "    print(f\"   ✓ Completed\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"results\": [result],\n",
    "        \"current_step\": state[\"current_step\"] + 1\n",
    "    }\n",
    "\n",
    "# Node 3: Generator (creates initial draft from results)\n",
    "def hybrid_generator(state: HybridState) -> dict:\n",
    "    \"\"\"Generate initial output from execution results.\"\"\"\n",
    "    if state[\"reflection_iterations\"] == 0:\n",
    "        # First generation from execution results\n",
    "        prompt = f\"\"\"Synthesize these step results into a cohesive response:\n",
    "\n",
    "Original task: {state['input']}\n",
    "\n",
    "Results from each step:\n",
    "{chr(10).join(state['results'])}\n",
    "\n",
    "Create a clear, complete response that addresses the original task.\"\"\"\n",
    "        print(\"\\n GENERATING INITIAL DRAFT FROM RESULTS...\")\n",
    "    else:\n",
    "        # Refinement based on critique\n",
    "        prompt = f\"\"\"Improve this draft based on the critique:\n",
    "\n",
    "Task: {state['input']}\n",
    "\n",
    "Current draft: {state['draft']}\n",
    "\n",
    "Critique: {state['critique']}\n",
    "\n",
    "Create an improved version.\"\"\"\n",
    "        print(f\"\\n REFINING DRAFT (iteration {state['reflection_iterations']})...\")\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"   ✓ Draft created\\n\")\n",
    "    \n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "# Node 4: Critic\n",
    "def hybrid_critic(state: HybridState) -> dict:\n",
    "    \"\"\"Critique the complete output.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate this response:\n",
    "\n",
    "Task: {state['input']}\n",
    "\n",
    "Response: {state['draft']}\n",
    "\n",
    "Provide constructive critique. What could be improved?\n",
    "If it's excellent, start with \"APPROVED:\".\n",
    "Otherwise, provide specific improvements needed.\"\"\"\n",
    "    \n",
    "    print(\" CRITIQUING DRAFT...\")\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    critique = response.content\n",
    "    \n",
    "    print(f\"   Critique: {critique[:150]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique,\n",
    "        \"reflection_iterations\": state[\"reflection_iterations\"] + 1\n",
    "    }\n",
    "\n",
    "# Node 5: Finalizer\n",
    "def hybrid_finalizer(state: HybridState) -> dict:\n",
    "    \"\"\"Finalize output.\"\"\"\n",
    "    print(\"\\n HYBRID AGENT COMPLETE!\\n\")\n",
    "    print(f\"Total reflection iterations: {state['reflection_iterations']}\\n\")\n",
    "    return {\"final_output\": state[\"draft\"]}\n",
    "\n",
    "print(\" Hybrid nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing functions\n",
    "def should_continue_execution(state: HybridState) -> Literal[\"executor\", \"generator\"]:\n",
    "    \"\"\"Check if more steps to execute.\"\"\"\n",
    "    if state[\"current_step\"] < len(state[\"plan\"]):\n",
    "        return \"executor\"\n",
    "    return \"generator\"\n",
    "\n",
    "def should_reflect_again(state: HybridState) -> Literal[\"generator\", \"finalizer\"]:\n",
    "    \"\"\"Check if refinement needed.\"\"\"\n",
    "    # Stop if approved\n",
    "    if \"APPROVED\" in state.get(\"critique\", \"\").upper():\n",
    "        print(\" Draft approved!\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    # Stop if max iterations\n",
    "    if state[\"reflection_iterations\"] >= MAX_REFLECTIONS:\n",
    "        print(f\"    Max reflection iterations ({MAX_REFLECTIONS}) reached\\n\")\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    return \"generator\"\n",
    "\n",
    "# Build hybrid graph\n",
    "hybrid_builder = StateGraph(HybridState)\n",
    "\n",
    "hybrid_builder.add_node(\"planner\", hybrid_planner)\n",
    "hybrid_builder.add_node(\"executor\", hybrid_executor)\n",
    "hybrid_builder.add_node(\"generator\", hybrid_generator)\n",
    "hybrid_builder.add_node(\"critic\", hybrid_critic)\n",
    "hybrid_builder.add_node(\"finalizer\", hybrid_finalizer)\n",
    "\n",
    "# Plan-Execute flow\n",
    "hybrid_builder.add_edge(START, \"planner\")\n",
    "hybrid_builder.add_edge(\"planner\", \"executor\")\n",
    "hybrid_builder.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    should_continue_execution,\n",
    "    {\"executor\": \"executor\", \"generator\": \"generator\"}\n",
    ")\n",
    "\n",
    "# Reflection flow\n",
    "hybrid_builder.add_edge(\"generator\", \"critic\")\n",
    "hybrid_builder.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_reflect_again,\n",
    "    {\"generator\": \"generator\", \"finalizer\": \"finalizer\"}\n",
    ")\n",
    "\n",
    "hybrid_builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "hybrid_agent = hybrid_builder.compile()\n",
    "\n",
    "print(\"Hybrid agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph\n",
    "try:\n",
    "    display(Image(hybrid_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Hybrid Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the specified scenario\n",
    "result = hybrid_agent.invoke({\n",
    "    \"input\": \"Research the benefits of Python programming, create a summary, and make it beginner-friendly\",\n",
    "    \"plan\": [],\n",
    "    \"current_step\": 0,\n",
    "    \"results\": [],\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"reflection_iterations\": 0\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL OUTPUT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result[\"final_output\"])\n",
    "print(f\"{'='*70}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
